{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# How to exploit data on Pangeo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Pangeo Workshop - Snow and Cloud Cover\n",
    "converted from https://github.com/EO-College/cubes-and-clouds/blob/main/lectures/3.1_data_processing/exercises/_alternatives/31_data_processing_stac.ipynb\n",
    "\n",
    "original author: Michele Clous @clausmichele\n",
    "conversion by: Pangeo volunteers (Pier Lorenzo Marasco @pl-marasco, Alejandro Coca-Castro @acocac, Anne Fouilloux @annefou, Justus Magin @keewis, Tina Odaka @tinaok)\n",
    "\n",
    "#### Introduction\n",
    "In this exercise, we will build a complete the same EO workflow as OpenEO using cloud provided data (STAC Catalogue), processing it locally; from data access to obtaining the result.\n",
    "\n",
    "We are going to follow these steps in our analysis:\n",
    "\n",
    "- Load satellite collections\n",
    "- Specify the spatial, temporal extents and the features we are interested in\n",
    "- Process the satellite data to retrieve snow cover information\n",
    "- aggregate information in data cubes\n",
    "- Visualize and analyse the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "###\n",
    "Important Infos \n",
    "\n",
    "More information on Pangeo can be found here: https://pangeo.io/\n",
    "More information on the STAC specification can be found here: https://stacspec.org/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data Manipulation and Analysis Libraries\n",
    "import pandas as pd  \n",
    "import numpy as np \n",
    "\n",
    "# Geospatial Data Handling Libraries\n",
    "import geopandas as gpd \n",
    "from shapely.geometry import mapping  \n",
    "import pyproj\n",
    "\n",
    "# Multidimensional and Satellite Data Libraries\n",
    "import xarray as xr \n",
    "import rioxarray as rio\n",
    "import stackstac\n",
    "\n",
    "# Data Visualization Libraries\n",
    "import holoviews as hv\n",
    "import hvplot.xarray\n",
    "import hvplot.pandas\n",
    "\n",
    "# Data parallelization and distributed computing libraries\n",
    "import dask\n",
    "from dask.distributed import Client, progress, LocalCluster\n",
    "\n",
    "# STAC Catalogue Libraries\n",
    "import pystac_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Here we creates a Dask client, which is essential for managing and executing parallel computations efficiently in the subsequent parts of the notebook. There are situation where you can connect to a Dask Gateway, but for this exercise we will use a local cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster = LocalCluster()\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Client address can be copy and pasted to the dashboard to monitor the progress of the computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate snow cover with apply_ufunc\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <i class=\"fa-check-circle fa\" style=\"font-size: 22px;color:#666;\"></i> <b>Calculate snow cover using Xarray's apply_ufunc </b>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>The procedure for computing snow cover can be summed up as following python function.  \n",
    "</li> \n",
    "        <li>We first verify that Green, swir16 and scr are in the order of 0,1,2 th variable in band variable.   Then we simply copy and past all the python codes in a function.  </li>\n",
    "            </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_ndsi_snow_cloud(data):\n",
    "    green = data[0]\n",
    "    swir = data[1]\n",
    "    scl = data[2]\n",
    "    ndsi = (green - swir) / (green + swir)\n",
    "    ndsi_mask = ( ndsi > 0.4 )& ~np.isnan(ndsi)\n",
    "    snow = np.where(ndsi_mask, 1, ndsi)\n",
    "    snowmap = np.where((snow <= 0.42) & ~np.isnan(snow), 0, snow)\n",
    "    mask = ~( (scl == 8) | (scl == 9) | (scl == 3) )\n",
    "    snow_cloud = np.where(mask, snowmap, 2)\n",
    "    return snow_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "aoi = gpd.read_file('../data/catchment_outline.geojson', crs=\"EPGS:4326\")\n",
    "aoi_geojson = mapping(aoi.iloc[0].geometry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mask data before apply_ufunc\n",
    "<div class=\"alert alert-warning\">\n",
    "    <i class=\"fa-check-circle fa\" style=\"font-size: 22px;color:#666;\"></i> <b>Apply mask then persist the data, then apply_ufunc to perform computation.   </b>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>The masking procedure can be applied before the computation to not to download the data we do not use.     \n",
    "</li> \n",
    "            </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "URL = \"https://earth-search.aws.element84.com/v1\"\n",
    "catalog = pystac_client.Client.open(URL)\n",
    "items = catalog.search(\n",
    "    intersects=aoi_geojson,\n",
    "    collections=[\"sentinel-2-l2a\"],\n",
    "    datetime=\"2019-02-01/2019-06-10\"\n",
    ").item_collection()\n",
    "len(items)\n",
    "\n",
    "ds = stackstac.stack(items,\n",
    "                    bounds_latlon=aoi.iloc[0].geometry.bounds,\n",
    "                    resolution=20,\n",
    "                    chunksize=(1,-1,-1,-1),\n",
    "                    assets=['green', 'swir16', 'scl'])\n",
    "\n",
    "aoi_utm32 = aoi.to_crs(epsg=32632)\n",
    "geom_utm32 = aoi_utm32.iloc[0]['geometry']\n",
    "ds.rio.write_crs(\"EPSG:32632\", inplace=True)\n",
    "ds.rio.set_nodata(np.nan, inplace=True)\n",
    "ds = ds.rio.clip([geom_utm32]).persist()\n",
    "ds\n",
    "\n",
    "snow_cloud=xr.apply_ufunc(\n",
    "    calculate_ndsi_snow_cloud\n",
    "    ,ds\n",
    "    ,input_core_dims=[[\"band\",\"y\",\"x\"]]\n",
    "    ,output_core_dims=[[\"y\",\"x\"]]\n",
    "    ,exclude_dims=set([\"band\"])\n",
    "    ,vectorize=True\n",
    "    ,dask=\"parallelized\"\n",
    "    ,output_dtypes=[ds.dtype]\n",
    "    ).assign_attrs({'long_name': 'snow_cloud'}).to_dataset(name='snow_cloud')\n",
    "\n",
    "\n",
    "snowmap_clipped=snow_cloud\n",
    "\n",
    "clipped_date = snowmap_clipped.groupby(snowmap_clipped.time.dt.floor('D')).max(skipna=True).rename({'floor': 'date'})\n",
    "\n",
    "#clipped_date = clipped_date.compute()\n",
    "clipped_date\n",
    "\n",
    "\n",
    "test=clipped_date\n",
    "\n",
    "\n",
    "def remove_attrs(obj, to_remove):\n",
    "    new = obj.copy()\n",
    "    new.attrs = {k: v for k, v in obj.attrs.items() if k not in to_remove}\n",
    "\n",
    "    return new\n",
    "\n",
    "def encode(obj):\n",
    "    object_coords = [name for name, coord in obj.coords.items() if coord.dtype.kind == \"O\"]\n",
    "    return obj.drop_vars(object_coords).pipe(remove_attrs, [\"spec\", \"transform\"])\n",
    "\n",
    "\n",
    "test.pipe(encode).to_zarr('test_applyufunc.zarr',mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "clipped_date=xr.open_zarr('test_applyufunc.zarr').snow_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "clipped_date.hvplot.image(\n",
    "    x='x',\n",
    "    y='y',\n",
    "    groupby='date',\n",
    "    crs=pyproj.CRS.from_epsg(32632),\n",
    "    cmap='Pastel2',\n",
    "    clim=(-1, 2),\n",
    "    frame_width=500,\n",
    "    frame_height=500,\n",
    "    title='Snowmap',\n",
    "    geo=True, tiles='OSM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Compute statistics\n",
    "\n",
    "from the orinal notebook:\n",
    "Calculate Catchment Statistics\n",
    "We are looking at a region over time. We need to make sure that the information content meets our expected quality. Therefore, we calculate the cloud percentage for the catchment for each timestep. We use this information to filter the timeseries. All timesteps that have a cloud coverage of over 25% will be discarded.\n",
    "\n",
    "Ultimately we are interested in the snow covered area (SCA) within the catchment. We count all snow covered pixels within the catchment for each time step. Multiplied by the pixel size that would be the snow covered area. Divided the pixel count by the total number of pixels in the catchment is the percentage of pixels covered with snow. We will use this number.\n",
    "\n",
    "Get number of pixels in catchment: total, clouds, snow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# number of cloud pixels\n",
    "cloud = xr.where(clipped_date == 2, 1, np.nan).count(dim=['x', 'y']).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# number of all pixels per each single date\n",
    "aot_total = clipped_date.count(dim=['x', 'y']).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Cloud fraction per each single date expressed in % \n",
    "cloud_fraction = (cloud / aot_total * 100).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Visualize cloud fraction\n",
    "cloud_fraction.hvplot.line(title='Cloud cover %', ylabel=\"&\") * hv.HLine(25).opts(\n",
    "    color='red',\n",
    "    line_dash='dashed',\n",
    "    line_width=2.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We are going to get the same information for the snow cover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "snow = xr.where(clipped_date == 1, 1, np.nan).count(dim=['x', 'y']).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "snow_fraction = (snow / aot_total * 100).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# viaualize snow fraction\n",
    "snow_fraction.hvplot.line(title='Snow cover area (%)', ylabel=\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# mask out cloud fraction > 30% \n",
    "masked_cloud_fraction = cloud_fraction < 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "snow_selected = snow_fraction.sel(date=masked_cloud_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "snow_selected.name = 'SCA'\n",
    "snow_selected.hvplot.line(title=\"Snow fraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Let's compare the date with the discharge data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "discharge_ds = pd.read_csv('data/ADO_DSC_ITH1_0025.csv', sep=',', index_col='Time', parse_dates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Let's refine a little bit the data so that we can compare it with the snow cover data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "start_date = pd.to_datetime(\"2019/02/01\")\n",
    "end_date = pd.to_datetime(\"2019/06/30\")\n",
    "# filter discharge data to start and end dates\n",
    "discharge_ds = discharge_ds.loc[start_date:end_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "discharge_ds.discharge_m3_s.hvplot(title='Discharge volume', ylabel='Discharge (m$^3$/s)') * snow_selected.hvplot(ylabel='Snow cover area (%)')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "In this analysis, we have comprehensively examined the features, capabilities, and limitations of two prominent geospatial data processing frameworks: OpenEO and Pangeo. OpenEO offers a unified API that simplifies the process of accessing and processing earth observation data across various backends, allowing users to interact with different data sources seamlessly. Its standardized interface is a strong asset, making it accessible to a wide range of users, from researchers to application developers.\n",
    "\n",
    "On the other hand, Pangeo excels in facilitating big data geoscience. Its robust ecosystem, built around existing Python libraries like Dask and Xarray, makes it a powerful tool for large-scale data analysis and visualization. Pangeoâ€™s community-driven approach and open-source nature foster collaboration and innovation, promoting a dynamic and adaptable framework.\n",
    "\n",
    "Each platform has its own set of advantages and constraints. OpenEO simplifies interoperability and enhances accessibility, making it particularly beneficial for users who wish to work with diverse data sources without delving deeply into the complexities of each backend. Pangeo, with its emphasis on leveraging existing Python tools and libraries, is particularly potent for those comfortable with Python and seeking to perform extensive, scalable analyses.\n",
    "\n",
    "Choosing between OpenEO and Pangeo ultimately depends on the specific requirements and constraints of a project. Considerations such as the user's familiarity with Python, the necessity for interoperability across various data backends, and the scale of data processing required should guide the decision-making process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
